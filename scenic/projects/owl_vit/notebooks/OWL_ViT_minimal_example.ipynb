{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satojkovic/scenic/blob/fix%2Frunnable-code/scenic/projects/owl_vit/notebooks/OWL_ViT_minimal_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaiG8Ulc75xc"
      },
      "source": [
        "# OWL-ViT minimal example\n",
        "\n",
        "This Colab shows how to **load a pre-trained OWL-ViT checkpoint** and use it to\n",
        "**get object detection predictions** for an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Yta1B7rtWu"
      },
      "source": [
        "# Download and install OWL-ViT\n",
        "\n",
        "OWL-ViT is implemented in [Scenic](https://github.com/google-research/scenic). The cell below installs the Scenic codebase from GitHub and imports it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWF7RkeZ4B_N"
      },
      "outputs": [],
      "source": [
        "!rm -rf *\n",
        "!rm -rf .config\n",
        "!rm -rf .git\n",
        "!git clone https://github.com/google-research/scenic.git .\n",
        "!python -m pip install -q .\n",
        "!python -m pip install -r ./scenic/projects/owl_vit/requirements.txt\n",
        "\n",
        "# Also install big_vision, which is needed for the mask head:\n",
        "!mkdir /big_vision\n",
        "!git clone https://github.com/google-research/big_vision.git /big_vision\n",
        "!python -m pip install -r /big_vision/big_vision/requirements.txt\n",
        "import sys\n",
        "sys.path.append('/big_vision/')\n",
        "!echo \"Done.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avoid AttributeError: module 'jax.random' has no attribute 'PRNGKeyArray'"
      ],
      "metadata": {
        "id": "gHs7LNG3T1Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ott-jax==0.2.0 --no-dependencies"
      ],
      "metadata": {
        "id": "vamMFND8T0t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MKZb6G3-H92"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import jax\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from scenic.projects.owl_vit import configs\n",
        "from scenic.projects.owl_vit import models\n",
        "from scipy.special import expit as sigmoid\n",
        "import skimage\n",
        "from skimage import io as skimage_io\n",
        "from skimage import transform as skimage_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WatINO87evx"
      },
      "source": [
        "# Choose config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4RKu3Vv5k_3"
      },
      "outputs": [],
      "source": [
        "config = configs.owl_v2_clip_b16.get_config(init_mode='canonical_checkpoint')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c12cyRK7oOD"
      },
      "source": [
        "# Load the model and variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s421Kpp7sXjD"
      },
      "outputs": [],
      "source": [
        "module = models.TextZeroShotDetectionModule(\n",
        "    body_configs=config.model.body,\n",
        "    objectness_head_configs=config.model.objectness_head,\n",
        "    normalize=config.model.normalize,\n",
        "    box_bias=config.model.box_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmaY8tQ23nJ3"
      },
      "outputs": [],
      "source": [
        "variables = module.load_variables(config.init_from.checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Knbjoxy2zW"
      },
      "source": [
        "# Prepare image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avoid FileNotFoundError: No such file: '/root/.cache/scikit-image/0.25.2/data/astronaut.png'"
      ],
      "metadata": {
        "id": "4GuDbaCHWxqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skimage.data.download_all(directory=None)"
      ],
      "metadata": {
        "id": "7AyDgL9rVCcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ilV_T2RyNT"
      },
      "outputs": [],
      "source": [
        "# Load example image:\n",
        "filename = os.path.join(skimage.data_dir, 'astronaut.png')\n",
        "image_uint8 = skimage_io.imread(filename)\n",
        "image = image_uint8.astype(np.float32) / 255.0\n",
        "\n",
        "# Pad to square with gray pixels on bottom and right:\n",
        "h, w, _ = image.shape\n",
        "size = max(h, w)\n",
        "image_padded = np.pad(\n",
        "    image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
        "\n",
        "# Resize to model input size:\n",
        "input_image = skimage.transform.resize(\n",
        "    image_padded,\n",
        "    (config.dataset_configs.input_size, config.dataset_configs.input_size),\n",
        "    anti_aliasing=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJvG0eaYyplV"
      },
      "source": [
        "# Prepare text queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSDsqV0UxbtL"
      },
      "outputs": [],
      "source": [
        "text_queries = ['face', 'rocket', 'nasa badge', 'star-spangled banner']\n",
        "tokenized_queries = np.array([\n",
        "    module.tokenize(q, config.dataset_configs.max_query_length)\n",
        "    for q in text_queries\n",
        "])\n",
        "\n",
        "# Pad tokenized queries to avoid recompilation if number of queries changes:\n",
        "tokenized_queries = np.pad(\n",
        "    tokenized_queries,\n",
        "    pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
        "    constant_values=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdR3OpAIzAA0"
      },
      "source": [
        "# Get predictions\n",
        "This will take a minute on the first execution due to model compilation. Subsequent executions will be faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkU2rMjTrjtK"
      },
      "outputs": [],
      "source": [
        "jitted = jax.jit(module.apply, static_argnames=('train',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M16amaHdzGdK"
      },
      "outputs": [],
      "source": [
        "# Note: The model expects a batch dimension.\n",
        "predictions = jitted(\n",
        "    variables,\n",
        "    input_image[None, ...],\n",
        "    tokenized_queries[None, ...],\n",
        "    train=False)\n",
        "\n",
        "# Remove batch dimension and convert to numpy:\n",
        "predictions = jax.tree_util.tree_map(lambda x: np.array(x[0]), predictions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_hzCvxC1sKw"
      },
      "source": [
        "# Plot predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl6Lg0jc5cKY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZPdauOR2ZJ-"
      },
      "outputs": [],
      "source": [
        "score_threshold = 0.2\n",
        "\n",
        "logits = predictions['pred_logits'][..., :len(text_queries)]  # Remove padding.\n",
        "scores = sigmoid(np.max(logits, axis=-1))\n",
        "labels = np.argmax(predictions['pred_logits'], axis=-1)\n",
        "boxes = predictions['pred_boxes']\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for score, box, label in zip(scores, boxes, labels):\n",
        "  if score < score_threshold:\n",
        "    continue\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "          [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
        "  ax.text(\n",
        "      cx - w / 2,\n",
        "      cy + h / 2 + 0.015,\n",
        "      f'{text_queries[label]}: {score:1.2f}',\n",
        "      ha='left',\n",
        "      va='top',\n",
        "      color='red',\n",
        "      bbox={\n",
        "          'facecolor': 'white',\n",
        "          'edgecolor': 'red',\n",
        "          'boxstyle': 'square,pad=.3'\n",
        "      })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71-JnmJDGrMw"
      },
      "source": [
        "## Plot objectness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ8uGL9OGtyV"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "objectnesses = sigmoid(predictions['objectness_logits'])\n",
        "boxes = predictions['pred_boxes']\n",
        "\n",
        "objectness_threshold = np.partition(objectnesses, -top_k)[-top_k]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for box, objectness in zip(boxes, objectnesses):\n",
        "  if objectness < objectness_threshold:\n",
        "    continue\n",
        "\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "          [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
        "          color='lime')\n",
        "\n",
        "  ax.text(\n",
        "    cx - w / 2 + 0.015,\n",
        "    cy + h / 2 - 0.015,\n",
        "    f'{objectness:1.2f}',\n",
        "    ha='left',\n",
        "    va='bottom',\n",
        "    color='black',\n",
        "    bbox={\n",
        "        'facecolor': 'white',\n",
        "        'edgecolor': 'lime',\n",
        "        'boxstyle': 'square,pad=.3'\n",
        "    })\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)\n",
        "ax.set_title(f'Top {top_k} objects by objectness')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hhGqbZzVfX"
      },
      "source": [
        "# Image-conditioned detection\n",
        "This section shows how to use objects detected in one image as queries on other images, instead of text strings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy51i-eZzfou"
      },
      "source": [
        "## Prepare images\n",
        "* The query object will be taken from `source_image`.\n",
        "* Then, similar objects will be detected in `target_imge`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXInmLAvzhNv"
      },
      "outputs": [],
      "source": [
        "def prepare_image(name):\n",
        "  # Load example image:\n",
        "  filename = os.path.join(skimage.data_dir, name)\n",
        "  image_uint8 = skimage_io.imread(filename)\n",
        "  image = image_uint8.astype(np.float32) / 255.0\n",
        "\n",
        "  # Pad to square with gray pixels on bottom and right:\n",
        "  h, w, _ = image.shape\n",
        "  size = max(h, w)\n",
        "  image_padded = np.pad(\n",
        "      image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5\n",
        "  )\n",
        "\n",
        "  # Resize to model input size:\n",
        "  return skimage.transform.resize(\n",
        "      image_padded,\n",
        "      (config.dataset_configs.input_size, config.dataset_configs.input_size),\n",
        "      anti_aliasing=True,\n",
        "  )\n",
        "\n",
        "source_image = prepare_image('rocket.jpg')\n",
        "target_image = prepare_image('astronaut.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv3tBxY3_zkK"
      },
      "source": [
        "## Functions to call model components separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RjQvs1c_4j7"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "image_embedder = jax.jit(\n",
        "    functools.partial(\n",
        "        module.apply, variables, train=False, method=module.image_embedder\n",
        "    )\n",
        ")\n",
        "\n",
        "objectness_predictor = jax.jit(\n",
        "    functools.partial(\n",
        "        module.apply, variables, method=module.objectness_predictor\n",
        "    )\n",
        ")\n",
        "\n",
        "box_predictor = jax.jit(\n",
        "    functools.partial(module.apply, variables, method=module.box_predictor)\n",
        ")\n",
        "\n",
        "class_predictor = jax.jit(\n",
        "    functools.partial(module.apply, variables, method=module.class_predictor)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIV_4AX_0OZm"
      },
      "source": [
        "## Identify an object in the source image to use as query, and get its embedding\n",
        "\n",
        "Here, we show the top 3 predictions on the source image so that the user can select one to use as a query (we select the rocket here).\n",
        "\n",
        "To get a query embedding, it is necessary to use a box predicted by the model. We cannot directly embed a whole image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLt-t9kl1Wg7"
      },
      "outputs": [],
      "source": [
        "# Embedd images and get boxes, without text queries:\n",
        "feature_map = image_embedder(source_image[None, ...])\n",
        "\n",
        "b, h, w, d = feature_map.shape\n",
        "image_features = feature_map.reshape(b, h * w, d)\n",
        "\n",
        "objectnesses = objectness_predictor(image_features)['objectness_logits']\n",
        "\n",
        "source_boxes = box_predictor(\n",
        "    image_features=image_features, feature_map=feature_map\n",
        ")['pred_boxes']\n",
        "\n",
        "source_class_embeddings = class_predictor(image_features=image_features)[\n",
        "    'class_embeddings'\n",
        "]\n",
        "\n",
        "# Remove batch dimension\n",
        "objectnesses = np.array(objectnesses[0])\n",
        "source_boxes = np.array(source_boxes[0])\n",
        "source_class_embeddings = np.array(source_class_embeddings[0])\n",
        "\n",
        "top_k = 3\n",
        "objectnesses = sigmoid(objectnesses)\n",
        "objectness_threshold = np.partition(objectnesses, -top_k)[-top_k]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(source_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for i, (box, objectness) in enumerate(zip(source_boxes, objectnesses)):\n",
        "  if objectness < objectness_threshold:\n",
        "    continue\n",
        "\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot(\n",
        "      [cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "      [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
        "      color='lime',\n",
        "  )\n",
        "\n",
        "  ax.text(\n",
        "      cx - w / 2 + 0.015,\n",
        "      cy + h / 2 - 0.015,\n",
        "      f'Index {i}: {objectness:1.2f}',\n",
        "      ha='left',\n",
        "      va='bottom',\n",
        "      color='black',\n",
        "      bbox={\n",
        "          'facecolor': 'white',\n",
        "          'edgecolor': 'lime',\n",
        "          'boxstyle': 'square,pad=.3',\n",
        "      },\n",
        "  )\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)\n",
        "ax.set_title(f'Top {top_k} objects by objectness')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnCPvFFz1n9o"
      },
      "outputs": [],
      "source": [
        "# Get the query embedding with the index of the selected object.\n",
        "# We're using the rocket:\n",
        "query_object_index = 1527  # Index of the rocket box above.\n",
        "query_embedding = source_class_embeddings[query_object_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HUEJvP5304U"
      },
      "source": [
        "## Get predictions for target image with the query embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DruLkz-c37Gk"
      },
      "outputs": [],
      "source": [
        "feature_map = image_embedder(target_image[None, ...])\n",
        "\n",
        "b, h, w, d = feature_map.shape\n",
        "target_boxes = box_predictor(\n",
        "    image_features=feature_map.reshape(b, h * w, d), feature_map=feature_map\n",
        ")['pred_boxes']\n",
        "\n",
        "target_class_predictions = class_predictor(\n",
        "    image_features=feature_map.reshape(b, h * w, d),\n",
        "    query_embeddings=query_embedding[None, None, ...],  # [batch, queries, d]\n",
        ")\n",
        "\n",
        "\n",
        "# Remove batch dimension and convert to numpy:\n",
        "target_boxes = np.array(target_boxes[0])\n",
        "target_logits = np.array(target_class_predictions['pred_logits'][0])\n",
        "\n",
        "top_ind = np.argmax(target_logits[:, 0], axis=0)\n",
        "score = sigmoid(target_logits[top_ind, 0])\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(target_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "cx, cy, w, h = target_boxes[top_ind]\n",
        "ax.plot(\n",
        "    [cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
        "    color='lime',\n",
        ")\n",
        "\n",
        "ax.text(\n",
        "    cx - w / 2 + 0.015,\n",
        "    cy + h / 2 - 0.015,\n",
        "    f'Score: {score:1.2f}',\n",
        "    ha='left',\n",
        "    va='bottom',\n",
        "    color='black',\n",
        "    bbox={\n",
        "        'facecolor': 'white',\n",
        "        'edgecolor': 'lime',\n",
        "        'boxstyle': 'square,pad=.3',\n",
        "    },\n",
        ")\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)\n",
        "ax.set_title(f'Closest match')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0YHQWw0R5kG"
      },
      "source": [
        "# Benchmark inference speed\n",
        "This section shows how to benchmark the inference speed of OWL-ViT. Speed and accuracy can be traded off by reducing the input resolution. This is done by truncating the position embeddings, and it works if the model was trained with heavy size augmentation and padding at the bottom and/or right of the image. This is the case for **OWL-ViT v2**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LW8_T8lTtuM"
      },
      "outputs": [],
      "source": [
        "config = configs.owl_v2_clip_b16.get_config(init_mode='canonical_checkpoint')\n",
        "\n",
        "# Replace default checkpoint with one trained on O365+VG without prompts, for\n",
        "# comparability to the literature:\n",
        "config.init_from.checkpoint_path = 'gs://scenic-bucket/owl_vit/checkpoints/owl2-b16-960-st-ngrams-ft-o365vg_925e87d'\n",
        "\n",
        "# To use variable inference resolution, patch size and native (=training) grid\n",
        "# size need to be added to the config:\n",
        "config.model.body.patch_size = int(config.model.body.variant[-2:])\n",
        "config.model.body.native_image_grid_size = (\n",
        "    config.dataset_configs.input_size // config.model.body.patch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts2dTG-sbAym"
      },
      "outputs": [],
      "source": [
        "class PredictWithTextEmbeddings(models.TextZeroShotDetectionModule):\n",
        "  \"\"\"Module that performs box prediction with precomputed query embeddings.\"\"\"\n",
        "\n",
        "  def __call__(self, image, query_embeddings):\n",
        "    feature_map = self.image_embedder(image[None, ...], False)  # Add batch dim.\n",
        "    b, h, w, d = feature_map.shape\n",
        "    image_features = feature_map.reshape(b, h * w, d)\n",
        "    boxes = self.box_predictor(\n",
        "        image_features=image_features, feature_map=feature_map\n",
        "    )['pred_boxes']\n",
        "    logits = self.class_predictor(image_features, query_embeddings[None, ...])[\n",
        "        'pred_logits'\n",
        "    ]\n",
        "    return boxes, logits\n",
        "\n",
        "\n",
        "module = PredictWithTextEmbeddings(\n",
        "    body_configs=config.model.body,\n",
        "    objectness_head_configs=config.model.objectness_head,\n",
        "    normalize=config.model.normalize,\n",
        "    box_bias=config.model.box_bias,\n",
        ")\n",
        "\n",
        "variables = module.load_variables(config.init_from.checkpoint_path)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def predict(image, query_embeddings):\n",
        "  return module.apply(variables, image, query_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpYCp8yFUetb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Get fake query embeddings for benchmarking (1203 classes):\n",
        "embed_dim = models.clip_model.CONFIGS[config.model.body.variant]['embed_dim']\n",
        "query_embeddings = jax.random.normal(jax.random.PRNGKey(0), (1203, embed_dim))\n",
        "\n",
        "# Resolutions at which to benchmark the model:\n",
        "if config.model.body.patch_size == 16:\n",
        "  sizes = [368, 400, 448, 480, 528, 576, 624, 672, 736, 784, 848, 896, 960]\n",
        "else:\n",
        "  raise ValueError(\n",
        "      'Please define image sizes for patch size:'\n",
        "      f' {config.model.body.patch_size}'\n",
        "  )\n",
        "num_trials = 5\n",
        "all_timings = {}\n",
        "for image_size in sizes:\n",
        "  print(f'Benchmarking image size: {image_size}')\n",
        "\n",
        "  # Get fake image for benchmarking:\n",
        "  image = jax.random.uniform(jax.random.PRNGKey(0), (image_size, image_size, 3))\n",
        "  timings = []\n",
        "  for i in range(num_trials + 1):  # Add 1 trial to account for compilation.\n",
        "    start_time = time.time()\n",
        "    boxes, logits = predict(image, query_embeddings)\n",
        "    _ = jax.block_until_ready((boxes, logits))\n",
        "    timings.append(time.time() - start_time)\n",
        "\n",
        "  # Store the median. Note that the first trial will always be very slow due to\n",
        "  # model commpilation:\n",
        "  all_timings[image_size] = np.median(timings)\n",
        "  print(f'FPS at resolution={image_size}: {1/all_timings[image_size]:.2f}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUtgDL0erCwm"
      },
      "source": [
        "# Models with segmentation mask head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ljE_-irEje"
      },
      "source": [
        "## Load model with mask head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRkFxGydrCwn"
      },
      "outputs": [],
      "source": [
        "from scenic.projects.owl_vit.configs import clip_l14_with_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfVcbta9rCwn"
      },
      "outputs": [],
      "source": [
        "config = clip_l14_with_masks.get_config(init_mode='canonical_checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bFVP-3KrCwn"
      },
      "outputs": [],
      "source": [
        "module = models.TextZeroShotDetectionModule(\n",
        "    body_configs=config.model.body,\n",
        "    mask_head_configs=config.model.mask_head,\n",
        "    normalize=config.model.normalize,\n",
        "    box_bias=config.model.box_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDvwUne-rCwo"
      },
      "outputs": [],
      "source": [
        "variables = module.load_variables(config.init_from.checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA0qFkxPrCwo"
      },
      "outputs": [],
      "source": [
        "jitted = jax.jit(module.apply, static_argnames=('train',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eao4T1k-rCwo"
      },
      "outputs": [],
      "source": [
        "# Resize to model input size:\n",
        "input_image = skimage.transform.resize(\n",
        "    image_padded,\n",
        "    (config.dataset_configs.input_size, config.dataset_configs.input_size),\n",
        "    anti_aliasing=True)\n",
        "\n",
        "# Note: The model expects a batch dimension.\n",
        "predictions = jitted(\n",
        "    variables,\n",
        "    input_image[None, ...],\n",
        "    tokenized_queries[None, ...],\n",
        "    train=False)\n",
        "\n",
        "# Remove batch dimension and convert to numpy:\n",
        "predictions = jax.tree_util.tree_map(lambda x: np.array(x[0]), predictions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7uzmqqMrCwo"
      },
      "source": [
        "## Plot predictions, including masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS1_suIKrCwp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7onch1TrCwp"
      },
      "outputs": [],
      "source": [
        "score_threshold = 0.3\n",
        "\n",
        "logits = predictions['pred_logits'][..., :len(text_queries)]  # Remove padding.\n",
        "scores = sigmoid(np.max(logits, axis=-1))\n",
        "labels = np.argmax(predictions['pred_logits'], axis=-1)\n",
        "boxes = predictions['pred_boxes']\n",
        "\n",
        "masks = [None] * len(boxes)\n",
        "if 'pred_masks' in predictions:\n",
        "  masks = sigmoid(predictions['pred_masks'])\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for score, box, label, mask in zip(scores, boxes, labels, masks):\n",
        "  if score < score_threshold:\n",
        "    continue\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "          [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
        "\n",
        "  if mask is not None:\n",
        "    mask_img = plt.cm.viridis(mask)\n",
        "    mask_img[..., -1] = (mask > 0.5) * 0.8\n",
        "    extent = np.array((cx - w / 2, cx + w / 2, cy + h / 2, cy - h / 2))\n",
        "    ax.imshow(mask_img, extent=np.clip(extent, 0, 1))\n",
        "\n",
        "  ax.text(\n",
        "      cx - w / 2,\n",
        "      cy + h / 2 + 0.015,\n",
        "      f'{text_queries[label]}: {score:1.2f}',\n",
        "      ha='left',\n",
        "      va='top',\n",
        "      color='red',\n",
        "      bbox={\n",
        "          'facecolor': 'white',\n",
        "          'edgecolor': 'red',\n",
        "          'boxstyle': 'square,pad=.3'\n",
        "      })\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}