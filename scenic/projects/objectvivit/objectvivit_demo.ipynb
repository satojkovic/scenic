{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/projects/objectvivit/objectvivit_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ObjectViViT Demo: How can objects help action recognition?\n",
        "\n",
        "This notebook demonstrates ObjectViViT, a method that uses object detection results to improve action recognition in videos. The approach includes:\n",
        "\n",
        "1. **Object-guided token sampling**: Drop certain input tokens with minimal accuracy impact\n",
        "2. **Object-aware attention**: Enrich features with object information to improve recognition\n",
        "\n",
        "Paper: [How can objects help action recognition?](http://arxiv.org/abs/xxxx.xxxxx) (CVPR 2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cpu]\n",
        "!pip install -q flax\n",
        "!pip install -q ml-collections\n",
        "!pip install -q absl-py\n",
        "!pip install -q clu\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorflow-datasets\n",
        "!pip install -q opencv-python\n",
        "!pip install -q matplotlib\n",
        "!pip install -q numpy\n",
        "\n",
        "# Clone the scenic repository\n",
        "!git clone https://github.com/google-research/scenic.git\n",
        "%cd scenic\n",
        "\n",
        "# Install dmvr dependency\n",
        "!pip install -q git+https://github.com/deepmind/dmvr.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/scenic')\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ml_collections\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "# Import ObjectViViT modules\n",
        "from scenic.projects.objectvivit import model\n",
        "from scenic.projects.objectvivit import model_utils\n",
        "from scenic.projects.objectvivit.configs import ssv2_B16_object\n",
        "\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"JAX devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_config"
      },
      "source": [
        "## Model Configuration\n",
        "\n",
        "Let's load the default configuration for ObjectViViT on Something-Something v2 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# Get the default configuration\n",
        "config = ssv2_B16_object.get_config()\n",
        "\n",
        "print(\"Model configuration:\")\n",
        "print(f\"Model name: {config.model_name}\")\n",
        "print(f\"Image size: {config.dataset_configs.image_size}\")\n",
        "print(f\"Number of frames: {config.dataset_configs.num_frames}\")\n",
        "print(f\"Patch size: {config.model.patches.size}\")\n",
        "print(f\"Hidden size: {config.model.hidden_size}\")\n",
        "print(f\"Number of layers: {config.model.num_layers}\")\n",
        "print(f\"Number of heads: {config.model.num_heads}\")\n",
        "print(f\"Object attention enabled: {config.model.attention_config.object_attention}\")\n",
        "print(f\"Object sampling enabled: {config.model.attention_config.object_sampling}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_demo"
      },
      "source": [
        "## Model Demonstration\n",
        "\n",
        "Let's create a simplified demonstration of the ObjectViViT model components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model"
      },
      "outputs": [],
      "source": [
        "# Create model instance\n",
        "model_cls = model.ViViTModelWithObjects\n",
        "\n",
        "# Initialize with dummy data to understand the model structure\n",
        "rng = jax.random.PRNGKey(0)\n",
        "dummy_batch_size = 2\n",
        "num_frames = config.dataset_configs.num_frames\n",
        "image_size = config.dataset_configs.image_size\n",
        "\n",
        "# Create dummy input data\n",
        "dummy_video = jnp.ones((dummy_batch_size, num_frames, image_size, image_size, 3))\n",
        "dummy_objects = jnp.ones((dummy_batch_size, num_frames, 10, 4))  # 10 objects per frame, 4 coordinates\n",
        "dummy_object_features = jnp.ones((dummy_batch_size, num_frames, 10, 512))  # 512-dim features\n",
        "\n",
        "dummy_batch = {\n",
        "    'inputs': dummy_video,\n",
        "    'objects': dummy_objects,\n",
        "    'object_features': dummy_object_features,\n",
        "    'label': jnp.ones((dummy_batch_size,), dtype=jnp.int32)\n",
        "}\n",
        "\n",
        "print(f\"Input video shape: {dummy_video.shape}\")\n",
        "print(f\"Object bounding boxes shape: {dummy_objects.shape}\")\n",
        "print(f\"Object features shape: {dummy_object_features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_init"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "vivit_model_instance = model_cls(config, dataset_meta={\n",
        "    'num_classes': config.dataset_configs.num_classes,\n",
        "    'input_shape': (-1, num_frames, image_size, image_size, 3),\n",
        "    'num_train_examples': 168913,  # SSv2 train set size\n",
        "    'num_test_examples': 27157,    # SSv2 validation set size\n",
        "})\n",
        "\n",
        "print(\"ObjectViViT model initialized successfully!\")\n",
        "print(f\"Model type: {type(vivit_model_instance).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "key_features"
      },
      "source": [
        "## Key Features Explanation\n",
        "\n",
        "### 1. Object-Guided Token Sampling\n",
        "\n",
        "ObjectViViT uses object detection results to intelligently sample video tokens, keeping only the most relevant ones for action recognition. This reduces computational cost while maintaining accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sampling_demo"
      },
      "outputs": [],
      "source": [
        "# Demonstrate object-guided sampling concept\n",
        "def visualize_sampling_strategy():\n",
        "    # Create a mock visualization of token sampling\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    # Original tokens (all patches)\n",
        "    original_tokens = np.ones((8, 14, 14))  # 8 frames, 14x14 patches\n",
        "    original_tokens[:, 5:9, 5:9] = 2  # Highlight object regions\n",
        "    \n",
        "    # Sampled tokens (keeping object-relevant patches)\n",
        "    sampled_tokens = np.zeros((8, 14, 14))\n",
        "    # Keep object regions and some background\n",
        "    sampled_tokens[:, 4:10, 4:10] = original_tokens[:, 4:10, 4:10]\n",
        "    sampled_tokens[:, ::3, ::3] = original_tokens[:, ::3, ::3]  # Sparse background sampling\n",
        "    \n",
        "    # Visualize one frame\n",
        "    ax1.imshow(original_tokens[0], cmap='viridis', alpha=0.8)\n",
        "    ax1.set_title('Original Tokens (100%)')\n",
        "    ax1.set_xlabel('Spatial Dimension')\n",
        "    ax1.set_ylabel('Spatial Dimension')\n",
        "    \n",
        "    ax2.imshow(sampled_tokens[0], cmap='viridis', alpha=0.8)\n",
        "    ax2.set_title('Object-Guided Sampling (~40% tokens)')\n",
        "    ax2.set_xlabel('Spatial Dimension')\n",
        "    ax2.set_ylabel('Spatial Dimension')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Object-guided sampling keeps:\")\n",
        "    print(\"✓ All tokens around detected objects\")\n",
        "    print(\"✓ Sparse sampling of background regions\")\n",
        "    print(\"✓ Maintains ~66.2% accuracy with only 40% of tokens\")\n",
        "\n",
        "visualize_sampling_strategy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "object_attention_section"
      },
      "source": [
        "### 2. Object-Aware Attention\n",
        "\n",
        "The model enriches video features with object information through specialized attention mechanisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "attention_demo"
      },
      "outputs": [],
      "source": [
        "# Demonstrate object-aware attention concept\n",
        "def visualize_object_attention():\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "    \n",
        "    # Mock attention maps\n",
        "    frames = ['Frame 1', 'Frame 2', 'Frame 3']\n",
        "    \n",
        "    for i, frame_name in enumerate(frames):\n",
        "        # Standard attention (more diffuse)\n",
        "        standard_attention = np.random.beta(2, 5, (14, 14))\n",
        "        standard_attention = standard_attention / standard_attention.max()\n",
        "        \n",
        "        # Object-aware attention (focused on objects)\n",
        "        object_attention = np.random.beta(2, 5, (14, 14))\n",
        "        # Add object focus\n",
        "        object_regions = [(5, 7), (8, 10), (6, 9)]\n",
        "        for oy, ox in object_regions[:i+1]:\n",
        "            object_attention[oy-1:oy+2, ox-1:ox+2] += 0.8\n",
        "        object_attention = object_attention / object_attention.max()\n",
        "        \n",
        "        axes[0, i].imshow(standard_attention, cmap='hot', alpha=0.8)\n",
        "        axes[0, i].set_title(f'{frame_name}\\nStandard Attention')\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        axes[1, i].imshow(object_attention, cmap='hot', alpha=0.8)\n",
        "        axes[1, i].set_title(f'{frame_name}\\nObject-Aware Attention')\n",
        "        axes[1, i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Object-aware attention provides:\")\n",
        "    print(\"✓ Enhanced focus on object regions\")\n",
        "    print(\"✓ Better feature representation for actions\")\n",
        "    print(\"✓ Improved accuracy: 67.4% vs 66.1% baseline\")\n",
        "\n",
        "visualize_object_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## Results Summary\n",
        "\n",
        "ObjectViViT achieves strong results on Something-Something v2 dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results_viz"
      },
      "outputs": [],
      "source": [
        "# Results visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "configs = ['Baseline\\n(ViViT-B/16)', 'Object Sampling\\n(40% tokens)', 'Object Attention\\n(Full model)']\n",
        "accuracies = [66.1, 66.2, 67.4]\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(configs, accuracies, color=colors, alpha=0.8)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "            f'{acc}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Top-1 Accuracy (%)', fontsize=12)\n",
        "ax.set_title('ObjectViViT Results on Something-Something v2', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(65, 68)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Achievements:\")\n",
        "print(f\"• {accuracies[2] - accuracies[0]:.1f}% improvement over baseline\")\n",
        "print(f\"• Efficient inference with {100-40}% fewer tokens (sampling variant)\")\n",
        "print(f\"• State-of-the-art object-aware video understanding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_section"
      },
      "source": [
        "## Usage Instructions\n",
        "\n",
        "To use ObjectViViT for your own projects:\n",
        "\n",
        "1. **Install Scenic framework**:\n",
        "   ```bash\n",
        "   git clone https://github.com/google-research/scenic.git\n",
        "   cd scenic\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "2. **Install ObjectViViT dependencies**:\n",
        "   ```bash\n",
        "   pip install -r scenic/projects/objectvivit/requirements.txt\n",
        "   ```\n",
        "\n",
        "3. **Download pretrained checkpoints** (VideoMAE initialization)\n",
        "\n",
        "4. **Train the model**:\n",
        "   ```bash\n",
        "   python -m scenic.projects.objectvivit.main \\\n",
        "     --config=scenic/projects/objectvivit/configs/ssv2_B16_object.py \\\n",
        "     --workdir=your_workdir/\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "ObjectViViT demonstrates how object information can significantly improve video action recognition:\n",
        "\n",
        "- **Object-guided sampling** reduces computational cost while maintaining accuracy\n",
        "- **Object-aware attention** enhances feature representation\n",
        "- **Strong empirical results** on challenging Something-Something v2 dataset\n",
        "\n",
        "This approach opens up new directions for efficient and accurate video understanding.\n",
        "\n",
        "---\n",
        "\n",
        "**Citation:**\n",
        "```bibtex\n",
        "@inproceedings{zhou2023objects,\n",
        "  title={How can objects help action recognition?},\n",
        "  author={Zhou, Xingyi and Arnab, Anurag and Sun, Chen and Schmid, Cordelia},\n",
        "  booktitle={CVPR},\n",
        "  year={2023}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}